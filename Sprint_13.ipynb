{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Sprint_13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiguozhe01/DIC_Assignment/blob/master/Sprint_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2i86jFzxJKT"
      },
      "source": [
        "**Sprint ディープラーニングフレームワーク1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qy1xRM0xJKT"
      },
      "source": [
        "# 【問題1】スクラッチを振り返る\n",
        "\n",
        "* ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙しろ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oee9H6kFxJKU"
      },
      "source": [
        "**【解答】**\n",
        "\n",
        "1. 入力データの準備\n",
        "2. テストデータと訓練データに分ける。\n",
        "3. 入力データ内から、ランダムに「ミニバッチ」を取り出す\n",
        "4. 「バイアス」と「重み」を初期化する。\n",
        "5. 順伝搬を行う\n",
        "    1. 訓練データをパッド分に展開する。\n",
        "    2. 訓練データをフィルター（重み）に通して乗算する。\n",
        "    3. バイアスを加算する。\n",
        "    4. 活性化関数を行う\n",
        "    5. 全層を通過後、出力の際softmax関数を用いて、値を正規化する。\n",
        "    6. 出力した値を記録して、次のミニバッチの学習を行う。\n",
        "6. 逆伝搬を行う\n",
        "    1. 各重みパラメータに関する損失関数の勾配を求める\n",
        "\n",
        "7. 重みパラメータを勾配方向に微小量だけ更新する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFHmB-LYxgRM",
        "outputId": "d3f117e0-7258-46f5-e739-5cebb27d4155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "!python3 -m pip install tensorflow==2.3.0"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/ae/0b08f53498417914f2274cc3b5576d2b83179b0cbb209457d0fde0152174/tensorflow-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 50kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.12.1)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 41.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (0.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (2.10.0)\n",
            "Collecting tensorboard<3,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/1b/6a420d7e6ba431cf3d51b2a5bfa06a958c4141e3189385963dc7f6fbffb6/tensorboard-2.3.0-py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow==2.3.0) (50.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.2.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed tensorboard-2.3.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkFUoMDjxJKU"
      },
      "source": [
        "# 【問題2】スクラッチとTensorFlowの対応を考える"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fmxscj5xMw-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80M69dxFxJKV",
        "outputId": "e67a98bf-7182-4b0c-ace8-276f23950a90"
      },
      "source": [
        "\"\"\"\n",
        "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "# データセットの読み込み\n",
        "dataset_path =\"datasets_19_420_Iris.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "# データフレームから条件抽出\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "# ラベルを数値に変換\n",
        "y[y=='Iris-versicolor'] = 0\n",
        "y[y=='Iris-virginica'] = 1\n",
        "# print(y, y.shape, type(y))\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "# print(y, y.shape, type(y))\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 20\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "# 計算グラフに渡す引数の形を決める\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "# ネットワーク構造の読み込み                               \n",
        "logits = example_net(X)\n",
        "# 目的関数\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# 推定結果\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)    \n",
        "        total_loss = np.empty(total_batch)\n",
        "        # total_loss = 0\n",
        "        total_acc = np.empty(total_batch)\n",
        "        # total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= total_loss.mean()\n",
        "        #total_loss /= batch_size\n",
        "        total_acc /= total_acc.mean()\n",
        "        #total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.8f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : 7.44431210, val_loss : 12.3300, acc : 0.750, val_acc : 0.375\n",
            "Epoch 1, loss : 4.21767378, val_loss : 4.9650, acc : 0.750, val_acc : 0.625\n",
            "Epoch 2, loss : 13.51782227, val_loss : 3.4740, acc : 0.500, val_acc : 0.812\n",
            "Epoch 3, loss : 4.26804495, val_loss : 5.2629, acc : 0.750, val_acc : 0.562\n",
            "Epoch 4, loss : 5.67510748, val_loss : 2.3903, acc : 0.500, val_acc : 0.750\n",
            "Epoch 5, loss : 0.00879623, val_loss : 0.7658, acc : 1.000, val_acc : 0.875\n",
            "Epoch 6, loss : 0.00003815, val_loss : 0.0002, acc : 1.000, val_acc : 1.000\n",
            "Epoch 7, loss : 0.00000000, val_loss : 0.0039, acc : 1.000, val_acc : 1.000\n",
            "Epoch 8, loss : 0.00000000, val_loss : 0.0001, acc : 1.000, val_acc : 1.000\n",
            "Epoch 9, loss : 0.00000000, val_loss : 0.0013, acc : 1.000, val_acc : 1.000\n",
            "Epoch 10, loss : 0.00000000, val_loss : 0.0003, acc : 1.000, val_acc : 1.000\n",
            "Epoch 11, loss : 0.00000000, val_loss : 0.0185, acc : 1.000, val_acc : 1.000\n",
            "Epoch 12, loss : 0.00000000, val_loss : 0.1414, acc : 1.000, val_acc : 0.938\n",
            "Epoch 13, loss : 0.00000020, val_loss : 0.2350, acc : 1.000, val_acc : 0.875\n",
            "Epoch 14, loss : 0.00000001, val_loss : 0.8870, acc : 1.000, val_acc : 0.875\n",
            "Epoch 15, loss : 0.00003351, val_loss : 0.9285, acc : 1.000, val_acc : 0.875\n",
            "Epoch 16, loss : 0.34282634, val_loss : 4.9852, acc : 0.750, val_acc : 0.688\n",
            "Epoch 17, loss : 0.00999178, val_loss : 2.2108, acc : 1.000, val_acc : 0.750\n",
            "Epoch 18, loss : 0.00000249, val_loss : 2.7947, acc : 1.000, val_acc : 0.875\n",
            "Epoch 19, loss : 0.00000010, val_loss : 0.7657, acc : 1.000, val_acc : 0.812\n",
            "test_acc : 0.800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbBH2bE5xJKY"
      },
      "source": [
        "**【疑似コード】**\n",
        "\n",
        "**Comparision Between Scratch and TensorFlow(v1.15)**\n",
        "\n",
        "1. データセットの読込\n",
        "2. データフレームから条件の抽出\n",
        "3. データフレームから目的変数の抽出\n",
        "4. データフレームから説明変数の抽出\n",
        "5. pandas.dataframeからnumpy.arrayへ変換\n",
        "6. 目的変数を数値に変換\n",
        "7. 目的変数のdtypeをint型に変換して、2次元配列に再成形\n",
        "8. 説明変数と目的変数を訓練データと試験データに分別する。\n",
        "9. 分別した訓練データ（説明変数、目的変数）をさらに2分別する。\n",
        "----\n",
        "10. ランダムに10個データを一纏まりとしたミニバッチを行う関数を作成\n",
        "----\n",
        "11. ハイパーパラメータの設定\n",
        "    1. 学習率\n",
        "    2. バッチサイズ\n",
        "    3. エポック数\n",
        "    4. 第1隠れ層のノード数\n",
        "    5. 第2隠れ層のノード数\n",
        "    6. 入力データの列数（特徴数）\n",
        "    7. 入力データの行数（データ数）\n",
        "    8. クラス数\n",
        "12. **計算グラフに渡す引数の形を決める**\n",
        "13. ミニバッチ関数を用い、訓練データ用のバッチを作成してget_mini_batch_trainに格納\n",
        "----\n",
        "14. **3層ニューラルネットワークの順伝搬する関数を作成**\n",
        "    1. 工程11で設定したハイパーパラメータを用いて、重み3層とバイアス3つを作成\n",
        "    2. 1～2層目：入力データと重みの内積にバイアスを加算した後、活性化関数（relu）を適用\n",
        "    3. 出力層：2層目の出力と3つ目の重みの内積に3つ目のバイアスを加算して終了\n",
        "15. **ネットワーク構造の読込**\n",
        "    * 入力データが引数\n",
        "16. **目的関数：目的変数に損失関数を適用**\n",
        "    * シグモイド関数、交差エントロピー誤差を経て、数値の平均値を返す。\n",
        "17. **最適化手法を定義**\n",
        "    * Adam最適化法を指定の学習率でインスタンス化\n",
        "    * インスタンス化した目的関数を最小化し、最適化して返す。\n",
        "18. **推定結果の設定**\n",
        "    * tf.signを用いて目的変数とネットワーク構造化した後、シグモイド関数を適用した説明変数の符号が同一かを判定\n",
        "    * booliang型を返す\n",
        "19. **指標値計算の設定**\n",
        "    * シグモイド済みの推定結果（booliang型）をfloat32に変換して、平均値を返し、それを指標値とする。\n",
        "20. **variableの初期化**\n",
        "----\n",
        "21. **計算グラフの実行**\n",
        "    1. TensorFlow 1.x 特有のコマンドtf.Sessionを用いて計算グラフを実行する。\n",
        "    2. sess.run(init): 数値の初期化\n",
        "    3. 指定したエポック数の下、for文をループさせる。\n",
        "        1. バッチ合計 = 訓練データの行数分を指定バッチサイズで除算し、数値を切り上げし、intに変換する。\n",
        "        2. total_loss, total_accをゼロに初期化\n",
        "        3. 新たなfor文：get_mini_batch_trainに格納した要素とインデックスを同時に取得\n",
        "        4. 複数のイテラブル（変数名）mini_batch_x, mini_batch_yに要素取得して、ディクショナリー化\n",
        "        5. 上記の工程4の値と、最適化した損失関数と指標値を用いsess.runで実行\n",
        "        6. 損失と指標値を加算代入する。\n",
        "     4. 加算代入を終えたtotal_loss, total_accをサンプル数（データフレームの行数）で除算し、複合代入させる。\n",
        "     5. val_loss, val_accを算出するため、sess.run関数を実行（中身は最適化した損失関数と指標値、ディクショナリー化したX_val、y_val）\n",
        "     6. 各エポック毎の損失、val損失、指標値、val指標値を表示\n",
        "     7. 試験データと指標値で同じ計算グラフ廻して、評価値を出す。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVEfp7A1xJKY"
      },
      "source": [
        "# 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDsd5szOxJKY",
        "outputId": "83d87231-8c47-49ee-d5da-03a6172baa26"
      },
      "source": [
        "\"\"\"\n",
        "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを3値分類する\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "# データセットの読み込み\n",
        "dataset_path =\"datasets_19_420_Iris.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "# データフレームから条件抽出\n",
        "df =  df[(df[\"Species\"] == \"Iris-setosa\")|(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
        "# y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "# ラベルを数値に変換\n",
        "y = pd.get_dummies(df.Species, prefix='Species')\n",
        "# y[y=='Iris-versicolor'] = 0\n",
        "# y[y=='Iris-virginica'] = 1\n",
        "# y = y.astype(np.int)[:, np.newaxis]\n",
        "y = y.to_numpy()\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.01\n",
        "batch_size = 50\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "# 計算グラフに渡す引数の形を決める\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "# ネットワーク構造の読み込み                               \n",
        "logits = example_net(X)\n",
        "# 目的関数\n",
        "# loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# 推定結果\n",
        "# correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(tf.nn.softmax(logits), 1))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)    \n",
        "        total_loss = np.empty(total_batch)\n",
        "        # total_loss = 0\n",
        "        total_acc = np.empty(total_batch)\n",
        "        # total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= total_loss.mean()\n",
        "        #total_loss /= batch_size\n",
        "        total_acc /= total_acc.mean()\n",
        "        #total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.8f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : 126.65926361, val_loss : 123.8705, acc : 0.348, val_acc : 0.292\n",
            "Epoch 1, loss : 83.51921082, val_loss : 78.2784, acc : 0.239, val_acc : 0.375\n",
            "Epoch 2, loss : 33.43477249, val_loss : 33.5225, acc : 0.283, val_acc : 0.292\n",
            "Epoch 3, loss : 8.12497616, val_loss : 10.3206, acc : 0.630, val_acc : 0.625\n",
            "Epoch 4, loss : 22.82771301, val_loss : 20.3210, acc : 0.391, val_acc : 0.333\n",
            "Epoch 5, loss : 3.18047500, val_loss : 3.0254, acc : 0.652, val_acc : 0.708\n",
            "Epoch 6, loss : 0.71592098, val_loss : 1.4878, acc : 0.913, val_acc : 0.750\n",
            "Epoch 7, loss : 2.91247964, val_loss : 4.1004, acc : 0.717, val_acc : 0.583\n",
            "Epoch 8, loss : 2.74655461, val_loss : 4.9936, acc : 0.761, val_acc : 0.667\n",
            "Epoch 9, loss : 2.02749372, val_loss : 4.8485, acc : 0.826, val_acc : 0.667\n",
            "Epoch 10, loss : 0.67740995, val_loss : 2.3692, acc : 0.935, val_acc : 0.833\n",
            "Epoch 11, loss : 0.66403735, val_loss : 1.1853, acc : 0.870, val_acc : 0.833\n",
            "Epoch 12, loss : 0.39142495, val_loss : 1.1985, acc : 0.957, val_acc : 0.792\n",
            "Epoch 13, loss : 0.78264642, val_loss : 2.6339, acc : 0.891, val_acc : 0.833\n",
            "Epoch 14, loss : 0.71579975, val_loss : 2.4830, acc : 0.935, val_acc : 0.833\n",
            "Epoch 15, loss : 0.36287954, val_loss : 1.1605, acc : 0.957, val_acc : 0.792\n",
            "Epoch 16, loss : 0.41962877, val_loss : 1.0684, acc : 0.891, val_acc : 0.833\n",
            "Epoch 17, loss : 0.34550416, val_loss : 1.2940, acc : 0.957, val_acc : 0.875\n",
            "Epoch 18, loss : 0.57754618, val_loss : 2.1283, acc : 0.935, val_acc : 0.833\n",
            "Epoch 19, loss : 0.39922461, val_loss : 1.6172, acc : 0.957, val_acc : 0.875\n",
            "Epoch 20, loss : 0.28291401, val_loss : 0.9779, acc : 0.957, val_acc : 0.875\n",
            "Epoch 21, loss : 0.26493239, val_loss : 0.9246, acc : 0.957, val_acc : 0.875\n",
            "Epoch 22, loss : 0.29035839, val_loss : 1.2057, acc : 0.978, val_acc : 0.875\n",
            "Epoch 23, loss : 0.38253224, val_loss : 1.6000, acc : 0.957, val_acc : 0.875\n",
            "Epoch 24, loss : 0.30489996, val_loss : 1.3104, acc : 0.978, val_acc : 0.875\n",
            "Epoch 25, loss : 0.22337364, val_loss : 0.9694, acc : 0.978, val_acc : 0.917\n",
            "Epoch 26, loss : 0.20752949, val_loss : 0.9270, acc : 0.978, val_acc : 0.917\n",
            "Epoch 27, loss : 0.24728638, val_loss : 1.1098, acc : 0.978, val_acc : 0.917\n",
            "Epoch 28, loss : 0.29437512, val_loss : 1.3177, acc : 0.978, val_acc : 0.917\n",
            "Epoch 29, loss : 0.26490486, val_loss : 1.2138, acc : 0.978, val_acc : 0.917\n",
            "Epoch 30, loss : 0.20260711, val_loss : 0.9765, acc : 0.978, val_acc : 0.917\n",
            "Epoch 31, loss : 0.18245675, val_loss : 0.9120, acc : 0.978, val_acc : 0.917\n",
            "Epoch 32, loss : 0.20858984, val_loss : 1.0310, acc : 0.978, val_acc : 0.917\n",
            "Epoch 33, loss : 0.23155990, val_loss : 1.1390, acc : 0.978, val_acc : 0.917\n",
            "Epoch 34, loss : 0.20610657, val_loss : 1.0506, acc : 0.978, val_acc : 0.917\n",
            "Epoch 35, loss : 0.16241105, val_loss : 0.8884, acc : 0.978, val_acc : 0.917\n",
            "Epoch 36, loss : 0.15636167, val_loss : 0.8780, acc : 0.978, val_acc : 0.917\n",
            "Epoch 37, loss : 0.17812777, val_loss : 0.9799, acc : 0.978, val_acc : 0.917\n",
            "Epoch 38, loss : 0.17211622, val_loss : 0.9697, acc : 0.978, val_acc : 0.917\n",
            "Epoch 39, loss : 0.13784932, val_loss : 0.8450, acc : 0.978, val_acc : 0.917\n",
            "Epoch 40, loss : 0.12558167, val_loss : 0.8098, acc : 0.978, val_acc : 0.917\n",
            "Epoch 41, loss : 0.13897488, val_loss : 0.8771, acc : 0.978, val_acc : 0.917\n",
            "Epoch 42, loss : 0.12935027, val_loss : 0.8521, acc : 0.978, val_acc : 0.917\n",
            "Epoch 43, loss : 0.10209653, val_loss : 0.7566, acc : 0.978, val_acc : 0.917\n",
            "Epoch 44, loss : 0.10175575, val_loss : 0.7683, acc : 0.978, val_acc : 0.917\n",
            "Epoch 45, loss : 0.10456739, val_loss : 0.7933, acc : 0.978, val_acc : 0.917\n",
            "Epoch 46, loss : 0.08460695, val_loss : 0.7263, acc : 0.978, val_acc : 0.917\n",
            "Epoch 47, loss : 0.07440836, val_loss : 0.6985, acc : 0.978, val_acc : 0.917\n",
            "Epoch 48, loss : 0.07902300, val_loss : 0.7303, acc : 0.978, val_acc : 0.917\n",
            "Epoch 49, loss : 0.06370618, val_loss : 0.6808, acc : 0.978, val_acc : 0.917\n",
            "Epoch 50, loss : 0.05349325, val_loss : 0.6511, acc : 0.978, val_acc : 0.917\n",
            "Epoch 51, loss : 0.05719171, val_loss : 0.6795, acc : 0.978, val_acc : 0.917\n",
            "Epoch 52, loss : 0.04339949, val_loss : 0.6324, acc : 0.978, val_acc : 0.917\n",
            "Epoch 53, loss : 0.03755902, val_loss : 0.6181, acc : 0.978, val_acc : 0.917\n",
            "Epoch 54, loss : 0.03845932, val_loss : 0.6346, acc : 0.978, val_acc : 0.917\n",
            "Epoch 55, loss : 0.02797383, val_loss : 0.5946, acc : 0.978, val_acc : 0.917\n",
            "Epoch 56, loss : 0.02616645, val_loss : 0.5965, acc : 0.978, val_acc : 0.917\n",
            "Epoch 57, loss : 0.02431755, val_loss : 0.5976, acc : 0.978, val_acc : 0.917\n",
            "Epoch 58, loss : 0.01851949, val_loss : 0.5723, acc : 0.978, val_acc : 0.917\n",
            "Epoch 59, loss : 0.01830483, val_loss : 0.5818, acc : 0.978, val_acc : 0.917\n",
            "Epoch 60, loss : 0.01568803, val_loss : 0.5728, acc : 1.000, val_acc : 0.917\n",
            "Epoch 61, loss : 0.01321844, val_loss : 0.5618, acc : 1.000, val_acc : 0.917\n",
            "Epoch 62, loss : 0.01306440, val_loss : 0.5700, acc : 1.000, val_acc : 0.917\n",
            "Epoch 63, loss : 0.01097552, val_loss : 0.5578, acc : 1.000, val_acc : 0.917\n",
            "Epoch 64, loss : 0.01014297, val_loss : 0.5566, acc : 1.000, val_acc : 0.917\n",
            "Epoch 65, loss : 0.00969769, val_loss : 0.5589, acc : 1.000, val_acc : 0.917\n",
            "Epoch 66, loss : 0.00847120, val_loss : 0.5489, acc : 1.000, val_acc : 0.917\n",
            "Epoch 67, loss : 0.00824415, val_loss : 0.5521, acc : 1.000, val_acc : 0.917\n",
            "Epoch 68, loss : 0.00769785, val_loss : 0.5490, acc : 1.000, val_acc : 0.917\n",
            "Epoch 69, loss : 0.00713936, val_loss : 0.5439, acc : 1.000, val_acc : 0.917\n",
            "Epoch 70, loss : 0.00704545, val_loss : 0.5471, acc : 1.000, val_acc : 0.917\n",
            "Epoch 71, loss : 0.00660847, val_loss : 0.5418, acc : 1.000, val_acc : 0.917\n",
            "Epoch 72, loss : 0.00640960, val_loss : 0.5410, acc : 1.000, val_acc : 0.917\n",
            "Epoch 73, loss : 0.00630675, val_loss : 0.5420, acc : 1.000, val_acc : 0.917\n",
            "Epoch 74, loss : 0.00603573, val_loss : 0.5377, acc : 1.000, val_acc : 0.917\n",
            "Epoch 75, loss : 0.00598661, val_loss : 0.5391, acc : 1.000, val_acc : 0.917\n",
            "Epoch 76, loss : 0.00586766, val_loss : 0.5380, acc : 1.000, val_acc : 0.917\n",
            "Epoch 77, loss : 0.00573226, val_loss : 0.5358, acc : 1.000, val_acc : 0.917\n",
            "Epoch 78, loss : 0.00572414, val_loss : 0.5375, acc : 1.000, val_acc : 0.917\n",
            "Epoch 79, loss : 0.00561351, val_loss : 0.5355, acc : 1.000, val_acc : 0.917\n",
            "Epoch 80, loss : 0.00556436, val_loss : 0.5353, acc : 1.000, val_acc : 0.917\n",
            "Epoch 81, loss : 0.00554871, val_loss : 0.5360, acc : 1.000, val_acc : 0.917\n",
            "Epoch 82, loss : 0.00546479, val_loss : 0.5343, acc : 1.000, val_acc : 0.917\n",
            "Epoch 83, loss : 0.00545679, val_loss : 0.5352, acc : 1.000, val_acc : 0.917\n",
            "Epoch 84, loss : 0.00542223, val_loss : 0.5350, acc : 1.000, val_acc : 0.917\n",
            "Epoch 85, loss : 0.00536997, val_loss : 0.5341, acc : 1.000, val_acc : 0.917\n",
            "Epoch 86, loss : 0.00536924, val_loss : 0.5351, acc : 1.000, val_acc : 0.917\n",
            "Epoch 87, loss : 0.00532235, val_loss : 0.5344, acc : 1.000, val_acc : 0.917\n",
            "Epoch 88, loss : 0.00529443, val_loss : 0.5344, acc : 1.000, val_acc : 0.917\n",
            "Epoch 89, loss : 0.00528238, val_loss : 0.5349, acc : 1.000, val_acc : 0.917\n",
            "Epoch 90, loss : 0.00523487, val_loss : 0.5342, acc : 1.000, val_acc : 0.917\n",
            "Epoch 91, loss : 0.00521868, val_loss : 0.5346, acc : 1.000, val_acc : 0.917\n",
            "Epoch 92, loss : 0.00519007, val_loss : 0.5347, acc : 1.000, val_acc : 0.917\n",
            "Epoch 93, loss : 0.00515046, val_loss : 0.5343, acc : 1.000, val_acc : 0.917\n",
            "Epoch 94, loss : 0.00513301, val_loss : 0.5348, acc : 1.000, val_acc : 0.917\n",
            "Epoch 95, loss : 0.00509436, val_loss : 0.5345, acc : 1.000, val_acc : 0.917\n",
            "Epoch 96, loss : 0.00506326, val_loss : 0.5345, acc : 1.000, val_acc : 0.917\n",
            "Epoch 97, loss : 0.00503820, val_loss : 0.5347, acc : 1.000, val_acc : 0.917\n",
            "Epoch 98, loss : 0.00499746, val_loss : 0.5344, acc : 1.000, val_acc : 0.917\n",
            "Epoch 99, loss : 0.00497116, val_loss : 0.5347, acc : 1.000, val_acc : 0.917\n",
            "test_acc : 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJzCw3faxJKa"
      },
      "source": [
        "## 変更箇所\n",
        "\n",
        "* 13行目：Iris-setosaをdf[\"Species\"]に追加\n",
        "* 19行目：3値分類のため、on-hot encodingに対応するためpandasのget_dummies()を用いる\n",
        "    * よって、20~22行目までが不要となる。\n",
        "* 23行目：get_dummies()したため、yのデータ型式をpandas.coreからnumpyに戻す。\n",
        "    * to_numpy()を使用\n",
        "    * 16行目のy = np.array(y)でも同様の効果\n",
        "* 74行目：77行目のYに格納するplaceholderの列数を1から3に変えるため、n_classes = 3にする。\n",
        "* 105行目：sigmoid_cross_entropyからsoftmax_corss_entropyに変更\n",
        "* 111行目：多値用にtf.argmaxを使用。（複数の割合から最も高い値を返す）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ryWMJGFxJKb"
      },
      "source": [
        "# 【問題4】House Pricesのモデルを作成\n",
        "\n",
        "* 目的変数：SalePrice\n",
        "* 説明変数：GrLivArea, YearBuilt\n",
        "* 分類問題ではなく、回帰問題（目的変数を予測する）\n",
        "* DNNRegressor with Reluを用いる。\n",
        "* 隠れ層：デフォルト（2層：50ノード、100ノード）\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfD1bScOxJKb"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "house_data = pd.read_csv('/content/drive/My Drive/DIC/dataset/House_Prices/house_prices_train.csv')\n",
        "\n",
        "#GrLivArea、YearBuilt、SalePriceを抽出\n",
        "train = house_data.loc[:, ['GrLivArea', 'YearBuilt', 'SalePrice']]\n",
        "\n",
        "# DataFrameをndarrayに変換\n",
        "X = np.array(train.iloc[:, :-1])\n",
        "y = np.array(train.iloc[:, -1])[:,None]\n",
        "\n",
        "# 特徴量を標準化\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "# 目的変数を対数変換\n",
        "y = np.log1p(y)\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m_pWGoCxcSq"
      },
      "source": [
        "\"\"\"\n",
        "TensorFlowで実装したニューラルネットワークを使いHousePricesデータセットを多値分類する\n",
        "\"\"\"\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "# 計算グラフに渡す引数の形を決める\n",
        "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.compat.v1.placeholder(\"float\", [None, n_classes])\n",
        "# trainのミニバッチイテレータ\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "# ネットワーク構造の読み込み                               \n",
        "logits = example_net(X)\n",
        "# 目的関数\n",
        "loss_op = tf.reduce_mean(tf.squared_difference(Y, logits))\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# 推定結果（不要）\n",
        "# correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.squared_difference(Y, logits))\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, MSE : {:.3f}, val_MSE : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_MSE : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ZiDp4JxJKd"
      },
      "source": [
        "## 変更箇所\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fftgzk7kxJKd"
      },
      "source": [
        "# 【問題5】MNISTのモデルを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wy5aRfhxJKd"
      },
      "source": [
        "#《データセットをダウンロードするコード》\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "X_train = X_train[:,:,:,None]  # NHWC\n",
        "X_test = X_test[:,:,:,None]  # NHWC\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test = enc.transform(y_test[:, np.newaxis])\n",
        "\n",
        "# trainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G67m6RytxyU5"
      },
      "source": [
        "\"\"\"\n",
        "TensorFlowで実装したニューラルネットワークを使いMNISTデータセットを多値分類する\n",
        "\"\"\"\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "stride = 1\n",
        "pad = 'VALID'\n",
        "ksize = [3, 3]\n",
        "# 計算グラフに渡す引数の形を決める\n",
        "X = tf.placeholder(\"float\", [None, X_train.shape[1], X_train.shape[2], X_train.shape[3]])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([5,5,1,4])),  # HWCF\n",
        "        'w2': tf.Variable(tf.random_normal([3,3,4,16])),\n",
        "        'w3': tf.Variable(tf.random_normal([64, 32])),\n",
        "        'w4': tf.Variable(tf.random_normal([32, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([1,1,1,4])),\n",
        "        'b2': tf.Variable(tf.random_normal([1,1,1,16])),\n",
        "        'b3': tf.Variable(tf.random_normal([32])),\n",
        "        'b4': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.nn.conv2d(x, weights['w1'], stride, pad) + biases['b1']\n",
        "    layer_1 = tf.nn.max_pool2d(layer_1, ksize, ksize, pad)\n",
        "    layer_2 = tf.nn.conv2d(layer_1, weights['w2'], stride, pad) + biases['b2']\n",
        "    layer_2 = tf.nn.max_pool2d(layer_2, ksize, ksize, pad)\n",
        "    layer_2 = tf.layers.Flatten()(layer_2)\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
        "    layer_3 = tf.nn.relu(layer_3)\n",
        "    layer_output = tf.matmul(layer_3, weights['w4']) + biases['b4'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "# ネットワーク構造の読み込み                               \n",
        "logits = example_net(X)\n",
        "# 目的関数\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits), axis=0)\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# 推定結果\n",
        "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(tf.nn.softmax(logits), axis=1))\n",
        "# correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.nn.softmax(logits) - 0.5))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            _, loss, acc = sess.run([train_op, loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
        "    print(sess.run(tf.argmax(tf.nn.softmax(logits), 1), feed_dict={X: X_val, Y: y_val}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QTfQd_px0su"
      },
      "source": [
        "np.argmax(y_test, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}