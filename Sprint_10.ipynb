{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Sprint_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiguozhe01/DIC_Assignment/blob/master/Sprint_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7I7zzYen7X8"
      },
      "source": [
        "# Sprint 深層学習スクラッチ ディープニューラルネットワーク"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2J46Qjxn7X9"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4U5LijQoV60",
        "outputId": "931b8cda-b0dd-4b3d-cad6-3facec499949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "# 《サンプルコード1》\n",
        "# self.sigma : ガウス分布の標準偏差\n",
        "# self.lr : 学習率\n",
        "# self.n_nodes_prev : 1層目のノード数\n",
        "# self.n_nodes_self : 2層目のノード数\n",
        "# self.n_output : 出力層のノード数\n",
        "optimizer = SGD(self.lr)\n",
        "self.FC1 = FC(self.n_features, self.n_nodes_prev, SimpleInitializer(self.sigma), optimizer)\n",
        "self.activation1 = Tanh()\n",
        "self.FC2 = FC(self.n_nodes_prev, self.n_nodes_self, SimpleInitializer(self.sigma), optimizer)\n",
        "self.activation2 = Tanh()\n",
        "self.FC3 = FC(self.n_nodes_self, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
        "self.activation3 = Softmax()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cb1864d7cf51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# self.n_nodes_self : 2層目のノード数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# self.n_output : 出力層のノード数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_nodes_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleInitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SGD' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5VAFCPqo7GV",
        "outputId": "ed8588a4-11c8-4a1d-b2bf-f0f5706071b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "# 《サンプルコード2》\n",
        "# イテレーション毎のフォワード\n",
        "A1 = self.FC1.forward(X)\n",
        "Z1 = self.activation1.forward(A1)\n",
        "A2 = self.FC2.forward(Z1)\n",
        "Z2 = self.activation2.forward(A2)\n",
        "A3 = self.FC3.forward(Z2)\n",
        "Z3 = self.activation3.forward(A3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b23f9c415fc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 《サンプルコード2》\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# イテレーション毎のフォワード\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqEHEa-lo_V2",
        "outputId": "9ee771ec-596f-4ac1-d04b-fd970a81350f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "# 《サンプルコード３》\n",
        "# イテレーション毎のバックワード\n",
        "dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
        "dZ2 = self.FC3.backward(dA3)\n",
        "dA2 = self.activation2.backward(dZ2)\n",
        "dZ1 = self.FC2.backward(dA2)\n",
        "dA1 = self.activation1.backward(dZ1)\n",
        "dZ0 = self.FC1.backward(dA1) # dZ0は使用しない"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a6c8296f0c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 《サンプルコード３》\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# イテレーション毎のバックワード\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdA3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 交差エントロピー誤差とソフトマックスを合わせている\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u4FfQPCn7YA"
      },
      "source": [
        "## 【問題1】全結合層のクラス化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_IjWGAzn7YA"
      },
      "source": [
        "#Full Connected Layer Class\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    層の生成\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    combination : object instance\n",
        "      結合関数インスタンス\n",
        "    activation : object instance\n",
        "      活性化関数インスタンス\n",
        "    initializer : object instance\n",
        "      初期化方法のインスタンス\n",
        "    optimizer : object instance\n",
        "      最適化手法のインスタンス\n",
        "    n_nodes_prev : int\n",
        "      前の層のノード数\n",
        "    n_nodes_self : int\n",
        "      自身の層のノード数\n",
        "      \n",
        "    Attributes\n",
        "    ----------\n",
        "    w : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "      重みパラメータ\n",
        "    b : 次の形のndarray, shape (n_nodes_self, )\n",
        "      バイアスパラメータ\n",
        "    input : 次の形のndarray, shape (batch_size, n_nodes_prev)\n",
        "      入力データ\n",
        "    output : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      出力データ\n",
        "    prev : object instance\n",
        "      前の層\n",
        "    next : object instance\n",
        "      後の層\n",
        "    \"\"\"\n",
        "    def __init__(self, combination, activation, initializer, optimizer, n_nodes_prev, n_nodes_self):\n",
        "        self.comb = combination\n",
        "        self.activ = activation\n",
        "        self.initializer = initializer # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "        self.optimizer = optimizer\n",
        "        self.n_nodes_prev = n_nodes_prev\n",
        "        self.n_nodes_self = n_nodes_self\n",
        "        \n",
        "        self.w = self.initializer.W(self.n_nodes_prev, self.n_nodes_self)\n",
        "        self.b = self.initializer.B(self.n_nodes_self)\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "        self.prev = None\n",
        "        self.next = None\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_nodes_prev)\n",
        "            入力\n",
        "            \n",
        "        Returns\n",
        "        ----------\n",
        "        Z : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "            出力\n",
        "        \"\"\"        \n",
        "        A = self.comb.forward(X, self.w, self.b)\n",
        "        Z = self.activ.forward(A)\n",
        "        \n",
        "        self.input = X\n",
        "        self.output = Z\n",
        "        \n",
        "        if self.next:\n",
        "            return self.next.forward(Z)\n",
        "        else:\n",
        "            return Z\n",
        "    \n",
        "    def backward(self, y, lr):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "            後ろから流れてきた勾配\n",
        "        lr : float\n",
        "          学習率\n",
        "        \"\"\"\n",
        "        dA = self.activ.backward(y)\n",
        "        dz, dw, db = self.comb.backward(self.input, self.w, dA)\n",
        "\n",
        "        # パラメータ更新\n",
        "        self.w -= lr * self.optimizer.update_dw(self, dw)\n",
        "        self.b -= lr * self.optimizer.update_db(self, db)\n",
        "        \n",
        "        if self.prev:\n",
        "            self.prev.backward(dz, lr)\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "        \n",
        "def _connect_layers(self, layers):\n",
        "    \"\"\"\n",
        "    全層の結合\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layers : list\n",
        "      ネットワークに組み込まれる層のリスト\n",
        "    \"\"\"\n",
        "    for i, layer in enumerate(layers): \n",
        "\n",
        "        if i == 0:\n",
        "            layer.next = self.layers[i+1]\n",
        "\n",
        "        elif layer == self.layers[-1]:\n",
        "            layer.prev = self.layers[i-1]\n",
        "\n",
        "        else:\n",
        "            layer.next = self.layers[i+1]\n",
        "            layer.prev = self.layers[i-1]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgC8bjdvn7YC"
      },
      "source": [
        "## 【問題2】初期化方法のクラス化\n",
        "\n",
        "* 初期化を行うコードをクラス化する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrP0m4vln7YC"
      },
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def W(self, n_nodes_prev, n_nodes_self):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes_prev : int\n",
        "          前の層のノード数\n",
        "        n_nodes_self : int\n",
        "          自身の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes_prev, n_nodes_self)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes_self):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes_self : int\n",
        "          自身の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : 次の形のndarray, shape (n_nodes_self, )\n",
        "        \"\"\"\n",
        "        B = np.random.randn(n_nodes_self)\n",
        "        return B"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOrO9PhDn7YE"
      },
      "source": [
        "## 【問題3】最適化手法のクラス化\n",
        "\n",
        "* 最適化手法のクラス化を行う。\n",
        "\n",
        "* 確率的勾配降下法\n",
        "    * 1トレーニングサンプル毎にコスト関数の勾配を求め、重みを更新してやる方法\n",
        "    * 全トレーニングサンプルからコスト関数の重みを更新する従来の方法はバッチ勾配降下法と呼ばれている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QobVXRf5n7YF"
      },
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    確率的勾配降下法\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "        \n",
        "    def update_dw(self, layer, grad):\n",
        "        \"\"\"\n",
        "        ある層の重み勾配を渡す\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : instance object\n",
        "          更新前の層のインスタンス\n",
        "        grad : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "          重みの勾配\n",
        "        \"\"\"\n",
        "        return grad\n",
        "    \n",
        "    def update_db(self, layer, grad):\n",
        "        \"\"\"\n",
        "        ある層のバイアス勾配を渡す\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : instance object\n",
        "          更新前の層のインスタンス\n",
        "        grad : 次の形のndarray, shape (n_nodes_self, )\n",
        "          バイアスの勾配\n",
        "        \"\"\"\n",
        "        return grad"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ61wPdhn7YL"
      },
      "source": [
        "## 【問題4】活性化関数のクラス化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozd8-oy0n7YM"
      },
      "source": [
        "class Linear:\n",
        "    \"\"\"\n",
        "    線形結合\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    A_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      順伝播の出力\n",
        "    dZ_ : 次の形のndarray, shape (batch_size, n_nodes_prev)\n",
        "      逆伝播入力に対するdZ勾配\n",
        "    dw_ : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "      逆伝播入力に対するdw勾配\n",
        "    db_ : 次の形のndarray, shape (n_nodes_self, )\n",
        "      逆伝播入力に対するdb勾配\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.A_ = None\n",
        "        self.dZ_ = None\n",
        "        self.dw_ = None\n",
        "        self.db_ = None\n",
        "        \n",
        "    def forward(self, Z, w, b):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        Z : 次の形のndarray, shape (batch_size, n_nodes_prev)\n",
        "          ある層に順伝播された特徴量データ\n",
        "        w : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "          ある層の重み\n",
        "        b : 次の形のndarray, shape (n_nodes_self, )\n",
        "          ある層のバイアス\n",
        "        \"\"\"\n",
        "        self.A_ = Z @ w + b\n",
        "        \n",
        "        return self.A_\n",
        "    \n",
        "    def backward(self, Z, w, dA):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        Z : 次の形のndarray, shape (batch_size, n_nodes_prev)\n",
        "          ある層に順伝播された特徴量データ\n",
        "        w : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "          ある層の重み\n",
        "        dA : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に逆伝播されたAに関するLoss勾配\n",
        "        \"\"\"\n",
        "        self.dZ_ = dA @ w.T\n",
        "        self.dw_ = Z.T @ dA\n",
        "        self.db_ = np.sum(dA, axis=0)\n",
        "        \n",
        "        return self.dZ_, self.dw_, self.db_\n",
        "\n",
        "        \n",
        "class Sigmoid:\n",
        "    \"\"\"\n",
        "    シグモイド関数\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    Z_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      順伝播の出力\n",
        "    dA_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      逆伝播入力に対するdA勾配\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z_ = None\n",
        "        self.dA_ = None\n",
        "        \n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に順伝播された特徴量データ\n",
        "        \"\"\"\n",
        "        self.Z_ = 1 / (1+np.exp(-A))\n",
        "        \n",
        "        return self.Z_\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に逆伝播されたZに関するLoss勾配\n",
        "        \"\"\"\n",
        "        self.dA_ = dZ * ((1 - self.Z_) * self.Z_)\n",
        "    \n",
        "        return self.dA_\n",
        "        \n",
        "        \n",
        "class Tanh:\n",
        "    \"\"\"\n",
        "    ハイパーボリックタンジェント関数\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    Z_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      順伝播の出力\n",
        "    dA_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      逆伝播入力に対するdA勾配\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z_ = None\n",
        "        self.dA_ = None\n",
        "        \n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に順伝播された特徴量データ\n",
        "        \"\"\"\n",
        "        self.Z_ = np.tanh(A)\n",
        "        \n",
        "        return self.Z_\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に逆伝播されたZに関するLoss勾配\n",
        "        \"\"\"\n",
        "        self.dA_ = dZ * (1 - self.Z_**2)\n",
        "        \n",
        "        return self.dA_\n",
        "\n",
        "    \n",
        "class Softmax:\n",
        "    \"\"\"\n",
        "    SoftMax関数\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    Z_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      順伝播の出力\n",
        "    dA_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      逆伝播入力に対するdA勾配\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z_ = None\n",
        "        self.dA_ = None\n",
        "        \n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に順伝播された特徴量データ\n",
        "        \"\"\"\n",
        "        # オーバーフロー対策として定数を引き算する\n",
        "        C = np.max(A)\n",
        "        self.Z_ = np.exp(A - C) / np.sum(np.exp(A - C), axis=1)[:, None]\n",
        "        \n",
        "        return self.Z_\n",
        "    \n",
        "    def backward(self, y):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          正解ラベルデータ\n",
        "        \"\"\"\n",
        "        self.dA_ = self.Z_ - y\n",
        "        \n",
        "        return self.dA_"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAm4bkWrn7YN"
      },
      "source": [
        "## 【問題5】ReLUクラスの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TveDtU6Jn7YO"
      },
      "source": [
        "class ReLu:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        ReLu関数\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Z_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          順伝播の出力\n",
        "        dA_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          逆伝播入力に対するdA勾配\n",
        "        \"\"\"\n",
        "        self.Z_ = None\n",
        "        self.dA_ = None\n",
        "        \n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に順伝播された特徴量データ\n",
        "        \"\"\"\n",
        "        self.Z_ = np.maximum(A, 0)\n",
        "        \n",
        "        return self.Z_\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に逆伝播されたZに関するLoss勾配\n",
        "        \"\"\"\n",
        "        self.dA_ = dZ * np.where(self.Z_ > 0, 1, 0)\n",
        "        \n",
        "        return self.dA_"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRr6WEx6n7YQ"
      },
      "source": [
        "## 【問題6】重みの初期値\n",
        "\n",
        "* XavierInitializerクラスと、HeInitializerクラスを作成せよ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvkrneejn7YQ"
      },
      "source": [
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    ザビエルの初期値によるシンプルな初期化\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes_prev):\n",
        "        self.sigma = 1 / np.sqrt(n_nodes_prev)\n",
        "    \n",
        "    def W(self, n_nodes_prev, n_nodes_self):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes_prev : int\n",
        "          前の層のノード数\n",
        "        n_nodes_self : int\n",
        "          自身の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes_prev, n_nodes_self)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes_self):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes_self : int\n",
        "          自身の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : 次の形のndarray, shape (n_nodes_self, )\n",
        "        \"\"\"\n",
        "        B = np.random.randn(n_nodes_self)\n",
        "        return B\n",
        "    \n",
        "    \n",
        "class HeInitializer:\n",
        "    \"\"\"\n",
        "    フーの初期値によるシンプルな初期化\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes_prev):\n",
        "        self.sigma = np.sqrt(2/n_nodes_prev)\n",
        "        \n",
        "    def W(self, n_nodes_prev, n_nodes_self):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes_prev : int\n",
        "          前の層のノード数\n",
        "        n_nodes_self : int\n",
        "          自身の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes_prev, n_nodes_self)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes_self):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes_self : int\n",
        "          自身の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : 次の形のndarray, shape (n_nodes_self, )\n",
        "        \"\"\"\n",
        "        B = np.random.randn(n_nodes_self)\n",
        "        return B"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26DRfzE5n7YT"
      },
      "source": [
        "## 【問題7】最適化手法\n",
        "\n",
        "* 学習率を学習過程で変化させる、AdaGradのクラスを作成せよ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fTWC3R1n7YU"
      },
      "source": [
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGradによる確率的勾配降下法\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    Hw : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "      ある層のイテレーション毎の重み勾配の二乗和\n",
        "    Hb : 次の形のndarray, shape (n_nodes_self, )\n",
        "      ある層のイテレーション毎のバイアス勾配の二乗和\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Hw = 1e-8\n",
        "        self.Hb = 1e-8\n",
        "        \n",
        "    def update_dw(self, layer, grad):\n",
        "        \"\"\"\n",
        "        ある層の重み勾配を渡す\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : instance object\n",
        "          更新前の層のインスタンス\n",
        "        grad : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "          重みの勾配\n",
        "        \"\"\"\n",
        "        self.Hw += grad**2\n",
        "        grad *= (1/np.sqrt(self.Hw))\n",
        "        \n",
        "        return grad\n",
        "    \n",
        "    def update_db(self, layer, grad):\n",
        "        \"\"\"\n",
        "        ある層のバイアス勾配を渡す\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : instance object\n",
        "          更新前の層のインスタンス\n",
        "        grad : 次の形のndarray, shape (n_nodes_self, )\n",
        "          バイアスの勾配\n",
        "        \"\"\"\n",
        "        self.Hb += grad**2\n",
        "        grad *= (1/np.sqrt(self.Hb))\n",
        "        \n",
        "        return grad"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDhVFvUWn7YV"
      },
      "source": [
        "## 【問題8】クラスの完成\n",
        "\n",
        "* 任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成せよ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR8q2RmPn7YV"
      },
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    確率的勾配降下法\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "        \n",
        "    def update_dw(self, grad):\n",
        "        \"\"\"\n",
        "        ある層の重み勾配を渡す\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        grad : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "          重みの勾配\n",
        "        \"\"\"\n",
        "        return grad\n",
        "    \n",
        "    def update_db(self, grad):\n",
        "        \"\"\"\n",
        "        ある層のバイアス勾配を渡す\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        grad : 次の形のndarray, shape (n_nodes_self, )\n",
        "          バイアスの勾配\n",
        "        \"\"\"\n",
        "        return grad\n",
        "        \n",
        "\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGradによる確率的勾配降下法\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    Hw : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "      ある層のイテレーション毎の重み勾配の二乗和\n",
        "    Hb : 次の形のndarray, shape (n_nodes_self, )\n",
        "      ある層のイテレーション毎のバイアス勾配の二乗和\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Hw = 1e-8\n",
        "        self.Hb = 1e-8\n",
        "        \n",
        "    def update_dw(self, grad):\n",
        "        \"\"\"\n",
        "        ある層の重み勾配を渡す\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        grad : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "          重みの勾配\n",
        "        \"\"\"\n",
        "        self.Hw += grad**2\n",
        "        grad *= (1/np.sqrt(self.Hw))\n",
        "        \n",
        "        return grad\n",
        "    \n",
        "    def update_db(self, grad):\n",
        "        \"\"\"\n",
        "        ある層のバイアス勾配を渡す\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        grad : 次の形のndarray, shape (n_nodes_self, )\n",
        "          バイアスの勾配\n",
        "        \"\"\"\n",
        "        self.Hb += grad**2\n",
        "        grad *= (1/np.sqrt(self.Hb))\n",
        "        \n",
        "        return grad\n",
        "        \n",
        "\n",
        "class Linear:\n",
        "    \"\"\"\n",
        "    線形結合\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    A_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      順伝播の出力\n",
        "    dZ_ : 次の形のndarray, shape (batch_size, n_nodes_prev)\n",
        "      逆伝播入力に対するdZ勾配\n",
        "    dw_ : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "      逆伝播入力に対するdw勾配\n",
        "    db_ : 次の形のndarray, shape (n_nodes_self, )\n",
        "      逆伝播入力に対するdb勾配\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.A_ = None\n",
        "        self.dZ_ = None\n",
        "        self.dw_ = None\n",
        "        self.db_ = None\n",
        "        \n",
        "    def forward(self, Z, w, b):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        Z : 次の形のndarray, shape (batch_size, n_nodes_prev)\n",
        "          ある層に順伝播された特徴量データ\n",
        "        w : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "          ある層の重み\n",
        "        b : 次の形のndarray, shape (n_nodes_self, )\n",
        "          ある層のバイアス\n",
        "        \"\"\"\n",
        "        self.A_ = Z @ w + b\n",
        "        \n",
        "        return self.A_\n",
        "    \n",
        "    def backward(self, Z, w, dA):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        Z : 次の形のndarray, shape (batch_size, n_nodes_prev)\n",
        "          ある層に順伝播された特徴量データ\n",
        "        w : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "          ある層の重み\n",
        "        dA : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に逆伝播されたAに関するLoss勾配\n",
        "        \"\"\"\n",
        "        self.dZ_ = dA @ w.T\n",
        "        self.dw_ = Z.T @ dA\n",
        "        self.db_ = np.sum(dA, axis=0)\n",
        "        \n",
        "        return self.dZ_, self.dw_, self.db_\n",
        "\n",
        "        \n",
        "class Sigmoid:\n",
        "    \"\"\"\n",
        "    シグモイド関数\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    Z_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      順伝播の出力\n",
        "    dA_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      逆伝播入力に対するdA勾配\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z_ = None\n",
        "        self.dA_ = None\n",
        "        \n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に順伝播された特徴量データ\n",
        "        \"\"\"\n",
        "        self.Z_ = 1 / (1+np.exp(-A))\n",
        "        \n",
        "        return self.Z_\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に逆伝播されたZに関するLoss勾配\n",
        "        \"\"\"\n",
        "        self.dA_ = dZ * ((1 - self.Z_) * self.Z_)\n",
        "    \n",
        "        return self.dA_\n",
        "        \n",
        "        \n",
        "class Tanh:\n",
        "    \"\"\"\n",
        "    ハイパーボリックタンジェント関数\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    Z_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      順伝播の出力\n",
        "    dA_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      逆伝播入力に対するdA勾配\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z_ = None\n",
        "        self.dA_ = None\n",
        "        \n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に順伝播された特徴量データ\n",
        "        \"\"\"\n",
        "        self.Z_ = np.tanh(A)\n",
        "        \n",
        "        return self.Z_\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に逆伝播されたZに関するLoss勾配\n",
        "        \"\"\"\n",
        "        self.dA_ = dZ * (1 - self.Z_**2)\n",
        "        \n",
        "        return self.dA_\n",
        "        \n",
        "        \n",
        "class ReLu:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        ReLu関数\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Z_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          順伝播の出力\n",
        "        dA_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          逆伝播入力に対するdA勾配\n",
        "        \"\"\"\n",
        "        self.Z_ = None\n",
        "        self.dA_ = None\n",
        "        \n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に順伝播された特徴量データ\n",
        "        \"\"\"\n",
        "        self.Z_ = np.maximum(A, 0)\n",
        "        \n",
        "        return self.Z_\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に逆伝播されたZに関するLoss勾配\n",
        "        \"\"\"\n",
        "        self.dA_ = dZ * np.where(self.Z_ > 0, 1, 0)\n",
        "        \n",
        "        return self.dA_\n",
        "    \n",
        "        \n",
        "class Softmax:\n",
        "    \"\"\"\n",
        "    SoftMax関数\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    Z_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      順伝播の出力\n",
        "    dA_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      逆伝播入力に対するdA勾配\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.Z_ = None\n",
        "        self.dA_ = None\n",
        "        \n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          ある層に順伝播された特徴量データ\n",
        "        \"\"\"\n",
        "        # オーバーフロー対策として定数を引き算する\n",
        "        C = np.max(A)\n",
        "        self.Z_ = np.exp(A - C) / np.sum(np.exp(A - C), axis=1)[:, None]\n",
        "        \n",
        "        return self.Z_\n",
        "    \n",
        "    def backward(self, y):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "          正解ラベルデータ\n",
        "        \"\"\"\n",
        "        self.dA_ = self.Z_ - y\n",
        "        \n",
        "        return self.dA_\n",
        "                    \n",
        "        \n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def W(self, n_nodes_prev, n_nodes_self):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes_prev : int\n",
        "          前の層のノード数\n",
        "        n_nodes_self : int\n",
        "          自身の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes_prev, n_nodes_self)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes_self):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes_self : int\n",
        "          自身の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : 次の形のndarray, shape (n_nodes_self, )\n",
        "        \"\"\"\n",
        "        B = np.random.randn(n_nodes_self)\n",
        "        return B\n",
        "    \n",
        "\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    ザビエルの初期値によるシンプルな初期化\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes_prev):\n",
        "        self.sigma = 1 / np.sqrt(n_nodes_prev)\n",
        "    \n",
        "    def W(self, n_nodes_prev, n_nodes_self):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes_prev : int\n",
        "          前の層のノード数\n",
        "        n_nodes_self : int\n",
        "          自身の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes_prev, n_nodes_self)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes_self):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes_self : int\n",
        "          自身の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : 次の形のndarray, shape (n_nodes_self, )\n",
        "        \"\"\"\n",
        "        B = np.random.randn(n_nodes_self)\n",
        "        return B\n",
        "    \n",
        "    \n",
        "class HeInitializer:\n",
        "    \"\"\"\n",
        "    フーの初期値によるシンプルな初期化\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes_prev):\n",
        "        self.sigma = np.sqrt(2/n_nodes_prev)\n",
        "        \n",
        "    def W(self, n_nodes_prev, n_nodes_self):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes_prev : int\n",
        "          前の層のノード数\n",
        "        n_nodes_self : int\n",
        "          自身の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes_prev, n_nodes_self)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes_self):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes_self : int\n",
        "          自身の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : 次の形のndarray, shape (n_nodes_self, )\n",
        "        \"\"\"\n",
        "        B = np.random.randn(n_nodes_self)\n",
        "        return B\n",
        "    \n",
        "    \n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]\n",
        "    \n",
        "    \n",
        "class FC:\n",
        "    \"\"\"\n",
        "    層の生成\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    combination : object instance\n",
        "      結合関数インスタンス\n",
        "    activation : object instance\n",
        "      活性化関数インスタンス\n",
        "    initializer : object instance\n",
        "      初期化方法のインスタンス\n",
        "    optimizer : object instance\n",
        "      最適化手法のインスタンス\n",
        "    n_nodes_prev : int\n",
        "      前の層のノード数\n",
        "    n_nodes_self : int\n",
        "      自身の層のノード数\n",
        "      \n",
        "    Attributes\n",
        "    ----------\n",
        "    w : 次の形のndarray, shape (n_nodes_prev, n_nodes_self)\n",
        "      重みパラメータ\n",
        "    b : 次の形のndarray, shape (n_nodes_self, )\n",
        "      バイアスパラメータ\n",
        "    input : 次の形のndarray, shape (batch_size, n_nodes_prev)\n",
        "      入力データ\n",
        "    output : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "      出力データ\n",
        "    prev : object instance\n",
        "      前の層\n",
        "    next : object instance\n",
        "      後の層\n",
        "    \"\"\"\n",
        "    def __init__(self, combination, activation, initializer, optimizer, n_nodes_prev, n_nodes_self):\n",
        "        self.comb = combination\n",
        "        self.activ = activation\n",
        "        self.initializer = initializer # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "        self.optimizer = optimizer\n",
        "        self.n_nodes_prev = n_nodes_prev\n",
        "        self.n_nodes_self = n_nodes_self\n",
        "        \n",
        "        self.w = self.initializer.W(self.n_nodes_prev, self.n_nodes_self)\n",
        "        self.b = self.initializer.B(self.n_nodes_self)\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "        self.prev = None\n",
        "        self.next = None\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_nodes_prev)\n",
        "            入力\n",
        "            \n",
        "        Returns\n",
        "        ----------\n",
        "        Z : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "            出力\n",
        "        \"\"\"        \n",
        "        A = self.comb.forward(X, self.w, self.b)\n",
        "        Z = self.activ.forward(A)\n",
        "        \n",
        "        self.input = X\n",
        "        self.output = Z\n",
        "        \n",
        "        if self.next:\n",
        "            return self.next.forward(Z)\n",
        "        else:\n",
        "            return Z\n",
        "    \n",
        "    def backward(self, y, lr):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
        "            後ろから流れてきた勾配\n",
        "        lr : float\n",
        "          学習率\n",
        "        \"\"\"\n",
        "        dA = self.activ.backward(y)\n",
        "        dz, dw, db = self.comb.backward(self.input, self.w, dA)\n",
        "\n",
        "        # パラメータ更新\n",
        "        self.w -= lr * self.optimizer.update_dw(self, dw)\n",
        "        self.b -= lr * self.optimizer.update_db(self, db)\n",
        "        \n",
        "        if self.prev:\n",
        "            self.prev.backward(dz, lr)\n",
        "        else:\n",
        "            pass\n",
        "    \n",
        "    \n",
        "class ScratchDeepNeuralNetworkClassifier:\n",
        "    \"\"\"\n",
        "    可変層ニューラルネットワーク分類器\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    layers : list\n",
        "      ネットワークに組み込まれる層のリスト\n",
        "    epoch : int\n",
        "      エポック数\n",
        "    sigma : float\n",
        "      初期パラメータ用（SimpleInitializerのみ適用）\n",
        "    batch_size : int\n",
        "      ミニバッチのサンプル数\n",
        "    verbose : bool\n",
        "      学習経過の出力\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    loss_train : list\n",
        "      訓練データに対するLoss\n",
        "    loss_val : list\n",
        "      検証データに対するLoss\n",
        "    \"\"\"\n",
        "    def __init__(self, layers, epoch=100, sigma=0.1, lr=0.01, batch_size=100, verbose=False, **kwargs):\n",
        "        self.layers = layers\n",
        "        self.epoch = epoch\n",
        "        self.lr = lr\n",
        "        self.sigma = sigma\n",
        "        self.verbose = verbose\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_train = []\n",
        "        self.loss_val = []\n",
        "\n",
        "    def _connect_layers(self, layers):\n",
        "        \"\"\"\n",
        "        全層の結合\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        layers : list\n",
        "          ネットワークに組み込まれる層のリスト\n",
        "        \"\"\"\n",
        "        for i, layer in enumerate(layers): \n",
        "            \n",
        "            if i == 0:\n",
        "                layer.next = self.layers[i+1]\n",
        "                \n",
        "            elif layer == self.layers[-1]:\n",
        "                layer.prev = self.layers[i-1]\n",
        "                \n",
        "            else:\n",
        "                layer.next = self.layers[i+1]\n",
        "                layer.prev = self.layers[i-1]\n",
        "    \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        ニューラルネットワーク分類器を学習する。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            訓練データの特徴量\n",
        "        y : 次の形のndarray, shape (n_samples, n_classes)\n",
        "            訓練データの正解値\n",
        "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
        "            検証データの特徴量\n",
        "        y_val : 次の形のndarray, shape (n_samples, n_classes)\n",
        "            検証データの正解値\n",
        "        \"\"\"\n",
        "        # レイヤーインスタンスを作成\n",
        "        self._connect_layers(self.layers)    \n",
        "        \n",
        "        for i in range(self.epoch):\n",
        "            \n",
        "            get_mini_batch_t = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
        "            \n",
        "            times = []\n",
        "            start = time.time()\n",
        "            \n",
        "            # 各mini batchの損失をリスト化\n",
        "            loss_batch_t = []\n",
        "            \n",
        "            for X_mini, y_mini in get_mini_batch_t:\n",
        "\n",
        "                # 順伝播\n",
        "                output = self.layers[0].forward(X_mini)\n",
        "                # 逆伝播\n",
        "                self.layers[-1].backward(y_mini, self.lr)\n",
        "\n",
        "                loss_batch_t.append(self.cross_entropy(output, y_mini))\n",
        "            \n",
        "            # 各epochの平均損失をselfに格納\n",
        "            loss_train = np.mean(loss_batch_t)\n",
        "            self.loss_train.append(loss_train)\n",
        "            \n",
        "            \n",
        "            # 検証データの推定\n",
        "            if hasattr(X_val, '__array__') and hasattr(y_val, '__array__'):\n",
        "                \n",
        "                batch_size_v = int(self.batch_size * len(X_val)/len(X))\n",
        "                get_mini_batch_v = GetMiniBatch(X_val, y_val, batch_size=batch_size_v)\n",
        "                loss_batch_v = []\n",
        "\n",
        "                for X_mini, y_mini in get_mini_batch_v:\n",
        "                    \n",
        "                    output = self.layers[0].forward(X_mini)\n",
        "                \n",
        "                    loss_batch_v.append(self.cross_entropy(output, y_mini))\n",
        "            \n",
        "                # 各epochの平均損失をselfに格納\n",
        "                loss_val = np.mean(loss_batch_v)\n",
        "                self.loss_val.append(loss_val)\n",
        "\n",
        "            end = time.time()\n",
        "            times.append(end-start)\n",
        "\n",
        "            # 学習経過の出力\n",
        "            if self.verbose and (i+1) % 10 == 0:\n",
        "                print(\"Epoch {}; Loss {:.4f}\".format(i+1, loss_train),\n",
        "                      \"  --Avg Epoch Time {:.4f}sec\".format(np.mean(times)))            \n",
        "                   \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        ニューラルネットワーク分類器を使い推定する。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "          検証用データ\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "          次の形のndarray, shape (n_samples, )\n",
        "          推定結果\n",
        "        \"\"\"\n",
        "        output = self.layers[0].forward(X)\n",
        "        \n",
        "        return np.argmax(output, axis=1)\n",
        "        \n",
        "    def cross_entropy(self, X, y):\n",
        "        \"\"\"\n",
        "        クロスエントロピー誤差を計算\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_features)\n",
        "          入力データ\n",
        "        y : 次の形のndarray, shape (batch_size, n_classes)\n",
        "          入力データの正解ラベル\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "          float\n",
        "          クロスエントロピー誤差\n",
        "        \"\"\"\n",
        "        return (-1/len(X)) * np.sum((y*np.log(X)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny2eHLn3n7YX"
      },
      "source": [
        "## 【問題9】学習と推定\n",
        "\n",
        "* 層の数や活性化関数を変えたいくつかのネットワークを作成せよ。\n",
        "* MNISTのデータを学習・推定し、Accuracyを計算せよ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5QDTbs1n7YY",
        "outputId": "aa072c1d-8817-4fa3-d571-e5412b9c31b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#《データセットをダウンロードするコード》\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNxEMpI7p3my",
        "outputId": "a30fc7f4-4938-4c4f-f561-3d6989d737db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.max()) # 1.0\n",
        "print(X_train.min()) # 0.0"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHyKhRGXp6Ya",
        "outputId": "41b51c4c-194e-414f-c4a7-b3c033c81cf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "\n",
        "print(y_train_one_hot.shape) # (60000, 10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiq1mG0Rp9rQ",
        "outputId": "826d76b8-bf20-4c2d-c632-a2f4262ab5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
        "print(X_train.shape) # (48000, 784)\n",
        "print(X_val.shape) # (12000, 784)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(48000, 784)\n",
            "(12000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKzhpck8p_0i"
      },
      "source": [
        "def plot_loss(model, title='Scratch DNN Loss'):\n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(np.arange(len(model.loss_train)), model.loss_train, label='train loss')\n",
        "    plt.plot(np.arange(len(model.loss_val)), model.loss_val, label='val loss')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPEeIT2qCMI",
        "outputId": "698d6dcf-9f65-4830-ae0b-30a23935cffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc9rYI7OqEik"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 4層のSigmoidネットワーク（SGD最適化）\n",
        "layer_1 = FC(Linear(), Sigmoid(), XavierInitializer(784), SGD(), 784, 400)\n",
        "layer_2 = FC(Linear(), Sigmoid(), XavierInitializer(400), SGD(), 400, 200)\n",
        "layer_3 = FC(Linear(), Sigmoid(), XavierInitializer(200), SGD(), 200, 100)\n",
        "output = FC(Linear(), Softmax(), XavierInitializer(100), SGD(), 100, 10)\n",
        "\n",
        "params = {'epoch': 100, \n",
        "          'lr': 0.01,\n",
        "          'sigma':  0.01,\n",
        "          'batch_size': 200,\n",
        "          }\n",
        "\n",
        "nn_sig4 = ScratchDeepNeuralNetworkClassifier(layers=[layer_1, layer_2, layer_3, output], verbose=True, **params)\n",
        "\n",
        "nn_sig4.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "pred = nn_sig4.predict(X_test)\n",
        "\n",
        "print(\"\\n Accuracy: {}\".format(accuracy_score(y_test, pred)))\n",
        "\n",
        "plot_loss(nn_sig4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSRpL89nqHI4"
      },
      "source": [
        "# 4層のSigmoidネットワーク（AdaGrad最適化）\n",
        "layer_1 = FC(Linear(), Sigmoid(), XavierInitializer(784), AdaGrad(), 784, 400)\n",
        "layer_2 = FC(Linear(), Sigmoid(), XavierInitializer(400), AdaGrad(), 400, 200)\n",
        "layer_3 = FC(Linear(), Sigmoid(), XavierInitializer(200), AdaGrad(), 200, 100)\n",
        "output = FC(Linear(), Softmax(), XavierInitializer(100), AdaGrad(), 100, 10)\n",
        "\n",
        "params = {'epoch': 100, \n",
        "          'lr': 0.01,\n",
        "          'sigma':  0.01,\n",
        "          'batch_size': 200,\n",
        "          }\n",
        "\n",
        "nn_sig4 = ScratchDeepNeuralNetworkClassifier(layers=[layer_1, layer_2, layer_3, output], verbose=True, **params)\n",
        "\n",
        "nn_sig4.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "pred = nn_sig4.predict(X_test)\n",
        "\n",
        "print(\"\\n Accuracy: {}\".format(accuracy_score(y_test, pred)))\n",
        "\n",
        "plot_loss(nn_sig4)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}